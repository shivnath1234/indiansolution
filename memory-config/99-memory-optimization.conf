# FILE PATH: /etc/sysctl.d/99-memory-optimization.conf
# Memory Optimization Parameters for Pterodactyl + Game Servers
# Optimized for low-latency workloads with multi-tier swap hierarchy

# ============================================================================
# SWAP BEHAVIOR TUNING
# ============================================================================

# vm.swappiness: Controls swap usage tendency (0-100)
# 0   = Avoid swap at all costs (can lead to OOM kills)
# 1   = Minimal swapping (kernel will still swap to avoid OOM)
# 10  = Prefer RAM, only swap under memory pressure (recommended for game servers)
# 60  = Default balanced behavior
# 100 = Aggressive swapping
# 
# For game servers: 10-20 provides good balance between performance and OOM protection
vm.swappiness = 15

# vm.vfs_cache_pressure: Controls cache vs application memory preference (default: 100)
# <100 = Prefer keeping cache (good for I/O heavy workloads)
# 100  = Balanced behavior
# >100 = Prefer application memory over cache (good for memory-intensive applications)
#
# For game servers: 50-80 balances cache benefits with application memory needs
vm.vfs_cache_pressure = 60

# ============================================================================
# MEMORY PRESSURE AND OOM HANDLING
# ============================================================================

# vm.min_free_kbytes: Minimum free memory buffer (prevents OOM conditions)
# Too low: System becomes unresponsive under memory pressure
# Too high: Wastes memory that could be used by applications
# Formula: RAM_GB * 65536 (64MB per GB of RAM, minimum 128MB, maximum 1GB)
# This value is calculated dynamically by the optimization script
vm.min_free_kbytes = 262144

# vm.overcommit_memory: Memory overcommit policy
# 0 = Heuristic overcommit (default) - kernel estimates available memory
# 1 = Always allow overcommit - can lead to OOM kills but maximizes memory usage
# 2 = Strict accounting - never overcommit beyond RAM + swap
#
# For containers: 0 is usually best (allows flexibility while preventing extreme overcommit)
vm.overcommit_memory = 0

# vm.overcommit_ratio: Percentage of RAM + swap for overcommit (when mode = 2)
# Only used when overcommit_memory = 2
# 80 = Allow allocation up to 80% of RAM + swap
vm.overcommit_ratio = 80

# vm.oom_kill_allocating_task: Choose which process to kill during OOM
# 0 = Kill the process using the most memory (default)
# 1 = Kill the process that triggered the OOM condition
#
# For game servers: 1 is often better (kills the problematic process, not necessarily the largest)
vm.oom_kill_allocating_task = 1

# ============================================================================
# DIRTY PAGE HANDLING (DISK WRITE OPTIMIZATION)
# ============================================================================

# vm.dirty_ratio: Maximum percentage of RAM for dirty pages before blocking writes
# Lower values = more frequent writes, less memory used for buffers, lower latency
# Higher values = fewer writes, more memory for buffers, potential latency spikes
# Default: 20, Recommended for low-latency: 10-15
vm.dirty_ratio = 15

# vm.dirty_background_ratio: Percentage when background writeback starts
# Should be significantly lower than dirty_ratio
# Default: 10, Recommended for low-latency: 5-10
vm.dirty_background_ratio = 5

# vm.dirty_expire_centisecs: How long dirty pages can stay in memory (centiseconds)
# Lower values = more frequent writes, better crash recovery, lower memory usage
# Higher values = fewer writes, more memory for buffers
# Default: 3000 (30 seconds), Recommended for game servers: 1500 (15 seconds)
vm.dirty_expire_centisecs = 1500

# vm.dirty_writeback_centisecs: How often to wake up writeback threads (centiseconds)
# Lower values = more responsive writes, higher CPU overhead
# Higher values = less CPU overhead, potentially higher latency
# Default: 500 (5 seconds), Recommended for responsive systems: 250 (2.5 seconds)
vm.dirty_writeback_centisecs = 250

# ============================================================================
# MEMORY ALLOCATION AND FRAGMENTATION
# ============================================================================

# vm.zone_reclaim_mode: Memory reclaim behavior for NUMA systems
# 0 = Prefer allocation from other zones before reclaiming (recommended for most systems)
# 1 = Enable zone reclaim (can cause performance issues on multi-socket systems)
vm.zone_reclaim_mode = 0

# vm.extfrag_threshold: External fragmentation threshold (0-1000)
# Lower values trigger more memory compaction to reduce fragmentation
# Higher values allow more fragmentation but less compaction overhead
# Default: 500, Good balance for most workloads
vm.extfrag_threshold = 500

# ============================================================================
# CONTAINER AND CGROUP COMPATIBILITY
# ============================================================================

# vm.max_map_count: Maximum number of memory map areas per process
# Important for containers and some applications (especially Java-based games)
# Default: 65530, Recommended for containers: 262144
# Some applications (like Elasticsearch) may need even higher values
vm.max_map_count = 262144

# kernel.pid_max: Maximum process ID value
# Higher values allow more processes (important for container environments)
# Default: 32768, Recommended for containers: 4194304
kernel.pid_max = 4194304

# ============================================================================
# NETWORK BUFFER OPTIMIZATION
# ============================================================================
# These settings affect game server network performance

# net.core.rmem_max: Maximum receive buffer size per socket
# Important for game servers handling many connections
# Default: varies, Recommended: 16MB for high-performance networking
net.core.rmem_max = 16777216

# net.core.wmem_max: Maximum send buffer size per socket
# Important for game servers sending data to many clients
# Default: varies, Recommended: 16MB for high-performance networking
net.core.wmem_max = 16777216

# net.core.netdev_max_backlog: Maximum packets in input queue per CPU
# Higher values help with packet processing under load
# Default: 1000, Recommended for game servers: 5000
net.core.netdev_max_backlog = 5000

# ============================================================================
# ADDITIONAL PERFORMANCE TUNING
# ============================================================================

# kernel.sched_migration_cost_ns: Cost of migrating a task to another CPU (nanoseconds)
# Lower values = more aggressive migration, better load balancing
# Higher values = less migration, better cache locality
# Default: 500000 (0.5ms), Good for most workloads
kernel.sched_migration_cost_ns = 500000

# kernel.sched_autogroup_enabled: Automatic process grouping for better desktop responsiveness
# 0 = Disabled (better for servers)
# 1 = Enabled (better for desktop systems)
kernel.sched_autogroup_enabled = 0

# ============================================================================
# MONITORING AND DEBUGGING
# ============================================================================

# vm.stat_interval: How often to update /proc/vmstat (seconds)
# Lower values = more current statistics, higher overhead
# Higher values = less overhead, less current statistics
# Default: 1, Acceptable range: 1-10
vm.stat_interval = 1

# ============================================================================
# NOTES FOR ADMINISTRATORS
# ============================================================================
#
# To apply these settings immediately:
#   sysctl -p /etc/sysctl.d/99-memory-optimization.conf
#
# To view current values:
#   sysctl vm.swappiness
#   sysctl vm.vfs_cache_pressure
#   sysctl vm.min_free_kbytes
#
# To temporarily change a value for testing:
#   sysctl -w vm.swappiness=10
#
# Monitor the impact of these changes:
#   - Memory usage: free -h, cat /proc/meminfo
#   - Swap usage: swapon --show, cat /proc/swaps
#   - Memory pressure: vmstat 5, sar -r 5
#   - OOM events: dmesg | grep -i "killed process"
#   - Dirty pages: cat /proc/vmstat | grep dirty
#
# Adjust values based on your workload:
#   - More memory-intensive games: lower swappiness (5-10)
#   - More I/O-intensive workloads: lower vfs_cache_pressure (30-50)
#   - Systems with fast SSDs: higher dirty ratios (20-30)
#   - Systems with slow storage: lower dirty ratios (5-10)
#
# ============================================================================
